{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "507e973c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "import xgboost as xgb\n",
    "import psutil\n",
    "from statsmodels.tsa.stattools import acf, pacf, adfuller, q_stat\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "import matplotlib.pyplot as plt\n",
    "from pmdarima import auto_arima\n",
    "from prophet import Prophet\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from joblib import Parallel, delayed\n",
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c050044",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_CSV = r\"D:\\DEPI\\CIS project\\Train.csv\"\n",
    "TEST_CSV = r\"D:\\DEPI\\CIS project\\Test.csv\"\n",
    "SUB_CSV = r\"D:\\DEPI\\CIS project\\Submission.csv\"\n",
    "TRAIN_END = '2017-07-15'\n",
    "VAL_END = '2017-08-15'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "177b3ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading data...\")\n",
    "train = pd.read_csv(TRAIN_CSV)\n",
    "test = pd.read_csv(TEST_CSV)\n",
    "sub = pd.read_csv(SUB_CSV)\n",
    "for df in (train, test):\n",
    "    df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d' if df is train else '%d-%m-%Y', errors='coerce')\n",
    "    df[['store_nbr', 'onpromotion']] = df[['store_nbr', 'onpromotion']].astype('int32')\n",
    "    df['sales'] = df['sales'].astype('float32') if 'sales' in df else np.nan\n",
    "    df.dropna(subset=['date'], inplace=True)\n",
    "if train.empty:\n",
    "    raise ValueError(\"Train data is empty after preprocessing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489d5ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data...\n"
     ]
    }
   ],
   "source": [
    "train['is_train'] = 1\n",
    "test['is_train'] = 0\n",
    "combined = pd.concat([train, test]).sort_values(['store_nbr', 'family', 'date'])\n",
    "agg_dict = {col: 'first' for col in combined.columns if col not in ['store_nbr', 'family', 'date', 'sales', 'onpromotion', 'is_train']}\n",
    "agg_dict.update({'sales': 'mean', 'onpromotion': 'sum', 'is_train': 'first'})\n",
    "combined = combined.groupby(['store_nbr', 'family', 'date']).agg(agg_dict).reset_index()\n",
    "combined = combined.astype({\n",
    "    'store_nbr': 'int32',\n",
    "    'family': 'category',\n",
    "    'date': 'datetime64[ns]',\n",
    "    'sales': 'float32',\n",
    "    'onpromotion': 'int32',\n",
    "    'is_train': 'int8'\n",
    "})\n",
    "combined = combined.set_index(['store_nbr', 'family', 'date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc72dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handling missing values...\n"
     ]
    }
   ],
   "source": [
    "grouped = combined.groupby(['store_nbr', 'family'])\n",
    "processed_groups = []\n",
    "for (store_nbr, family), group in grouped:\n",
    "    # Reset index, removing 'store_nbr' and 'family' from index but not adding as columns yet\n",
    "    group = group.reset_index(['store_nbr', 'family'], drop=True)\n",
    "    group.index = pd.to_datetime(group.index)  # Ensure 'date' index is datetime\n",
    "    group = group[~group.index.duplicated()]  # Remove duplicate dates\n",
    "    \n",
    "    # Handle missing values\n",
    "    group['sales'] = group['sales'].ffill().fillna(0).astype('float32')\n",
    "    group['onpromotion'] = group['onpromotion'].fillna(0).astype('int32')\n",
    "    for col in group:\n",
    "        if col not in ['sales', 'onpromotion']:\n",
    "            group[col] = group[col].ffill().fillna(group[col].iloc[0] if not group[col].isna().all() else 0)\n",
    "    \n",
    "    # Add 'store_nbr' and 'family' as columns using the group key\n",
    "    group['store_nbr'] = store_nbr\n",
    "    group['family'] = family\n",
    "    \n",
    "    # Move 'date' from index to column\n",
    "    group = group.reset_index()\n",
    "    \n",
    "    processed_groups.append(group)\n",
    "\n",
    "# Concatenate all processed groups\n",
    "combined = pd.concat(processed_groups)\n",
    "\n",
    "# Set the index to ['store_nbr', 'family', 'date']\n",
    "combined = combined.set_index(['store_nbr', 'family', 'date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820247ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding features...\n"
     ]
    }
   ],
   "source": [
    "combined = combined.reset_index()\n",
    "combined['day'] = combined['date'].dt.day.astype('int8')\n",
    "combined['dow'] = combined['date'].dt.dayofweek.astype('int8')\n",
    "combined['is_weekend'] = combined['dow'].isin([5, 6]).astype('int8')\n",
    "combined['woy'] = combined['date'].dt.isocalendar().week.astype('int8')\n",
    "combined['month'] = combined['date'].dt.month.astype('int8')\n",
    "combined['quarter'] = combined['date'].dt.quarter.astype('int8')\n",
    "combined['year'] = combined['date'].dt.year.astype('int16')\n",
    "combined['sin_month'] = np.sin(2 * np.pi * combined['month'] / 12).astype('float32')\n",
    "combined['cos_month'] = np.cos(2 * np.pi * combined['month'] / 12).astype('float32')\n",
    "combined['promo_weekend'] = (combined['onpromotion'] * combined['is_weekend']).astype('int8')\n",
    "combined['description'] = combined['description'].astype(str)\n",
    "combined['is_holiday'] = combined['description'].str.contains(\"Holiday|Navidad|Carnaval\", case=False, na=False).astype('int8')\n",
    "\n",
    "lags = [1, 7, 14, 21, 28]\n",
    "windows = [7, 14, 21, 28]\n",
    "for lag in lags:\n",
    "    combined[f'lag_{lag}'] = combined.groupby(['store_nbr', 'family'])['sales'].shift(lag).astype('float32')\n",
    "for w in windows:\n",
    "    roll = combined.groupby(['store_nbr', 'family'])['sales'].shift(1).rolling(w, min_periods=1)\n",
    "    combined[f'roll_mean_{w}'] = roll.mean().astype('float32')\n",
    "    combined[f'roll_std_{w}'] = roll.std().astype('float32')\n",
    "combined['diff_1'] = combined.groupby(['store_nbr', 'family'])['sales'].diff(1).astype('float32')\n",
    "\n",
    "combined['store_nbr_encoded'] = LabelEncoder().fit_transform(combined['store_nbr']).astype('int8')\n",
    "combined['family_encoded'] = LabelEncoder().fit_transform(combined['family']).astype('int8')\n",
    "\n",
    "feature_cols = ['onpromotion', 'promo_weekend', 'is_holiday', 'day', 'dow', 'is_weekend', 'woy', 'month',\n",
    "                'quarter', 'year', 'sin_month', 'cos_month', 'store_nbr_encoded', 'family_encoded'] + \\\n",
    "               [f'lag_{lag}' for lag in lags] + [f'roll_mean_{w}' for w in windows] + \\\n",
    "               [f'roll_std_{w}' for w in windows] + ['diff_1']\n",
    "combined[feature_cols] = StandardScaler().fit_transform(combined[feature_cols].fillna(0)).astype('float32')\n",
    "combined = combined.set_index(['store_nbr', 'family', 'date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71543ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = combined[combined['is_train'] == 1].drop('is_train', axis=1)\n",
    "test = combined[combined['is_train'] == 0].drop(['is_train', 'sales'], axis=1)\n",
    "train_set = train[train.index.get_level_values('date') <= TRAIN_END]\n",
    "val_set = train[(train.index.get_level_values('date') > TRAIN_END) & (train.index.get_level_values('date') <= VAL_END)].dropna(subset=['sales'])\n",
    "if val_set.empty:\n",
    "    raise ValueError(\"Validation set is empty.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e845199c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training XGBoost...\n",
      "[0]\tvalidation_0-rmse:989.44318\n",
      "[1]\tvalidation_0-rmse:892.77377\n",
      "[2]\tvalidation_0-rmse:804.66383\n",
      "[3]\tvalidation_0-rmse:723.72571\n",
      "[4]\tvalidation_0-rmse:651.81157\n",
      "[5]\tvalidation_0-rmse:588.50992\n",
      "[6]\tvalidation_0-rmse:527.87906\n",
      "[7]\tvalidation_0-rmse:474.50653\n",
      "[8]\tvalidation_0-rmse:426.43887\n",
      "[9]\tvalidation_0-rmse:384.56282\n",
      "[10]\tvalidation_0-rmse:345.99290\n",
      "[11]\tvalidation_0-rmse:312.03650\n",
      "[12]\tvalidation_0-rmse:280.88012\n",
      "[13]\tvalidation_0-rmse:254.99973\n",
      "[14]\tvalidation_0-rmse:230.32120\n",
      "[15]\tvalidation_0-rmse:209.97231\n",
      "[16]\tvalidation_0-rmse:192.05279\n",
      "[17]\tvalidation_0-rmse:175.64734\n",
      "[18]\tvalidation_0-rmse:161.06883\n",
      "[19]\tvalidation_0-rmse:148.85556\n",
      "[20]\tvalidation_0-rmse:138.44808\n",
      "[21]\tvalidation_0-rmse:130.41651\n",
      "[22]\tvalidation_0-rmse:123.00202\n",
      "[23]\tvalidation_0-rmse:117.08262\n",
      "[24]\tvalidation_0-rmse:112.37035\n",
      "[25]\tvalidation_0-rmse:109.01658\n",
      "[26]\tvalidation_0-rmse:105.53972\n",
      "[27]\tvalidation_0-rmse:103.91466\n",
      "[28]\tvalidation_0-rmse:101.90643\n",
      "[29]\tvalidation_0-rmse:100.23211\n",
      "[30]\tvalidation_0-rmse:99.18971\n",
      "[31]\tvalidation_0-rmse:97.08920\n",
      "[32]\tvalidation_0-rmse:97.06717\n",
      "[33]\tvalidation_0-rmse:96.17969\n",
      "[34]\tvalidation_0-rmse:95.19939\n",
      "[35]\tvalidation_0-rmse:95.39576\n",
      "[36]\tvalidation_0-rmse:95.45900\n",
      "[37]\tvalidation_0-rmse:96.63361\n",
      "[38]\tvalidation_0-rmse:96.49416\n",
      "[39]\tvalidation_0-rmse:96.36347\n",
      "[40]\tvalidation_0-rmse:96.39930\n",
      "[41]\tvalidation_0-rmse:96.30235\n",
      "[42]\tvalidation_0-rmse:96.17426\n",
      "[43]\tvalidation_0-rmse:95.93071\n",
      "[44]\tvalidation_0-rmse:95.32856\n",
      "[45]\tvalidation_0-rmse:94.85894\n",
      "[46]\tvalidation_0-rmse:95.56484\n",
      "[47]\tvalidation_0-rmse:98.04589\n",
      "[48]\tvalidation_0-rmse:97.99594\n",
      "[49]\tvalidation_0-rmse:98.75052\n",
      "[50]\tvalidation_0-rmse:98.40974\n",
      "[51]\tvalidation_0-rmse:98.06327\n",
      "[52]\tvalidation_0-rmse:98.58103\n",
      "[53]\tvalidation_0-rmse:98.65642\n",
      "[54]\tvalidation_0-rmse:98.69082\n",
      "[55]\tvalidation_0-rmse:100.17819\n",
      "[56]\tvalidation_0-rmse:100.21803\n",
      "[57]\tvalidation_0-rmse:99.62137\n",
      "[58]\tvalidation_0-rmse:99.50331\n",
      "[59]\tvalidation_0-rmse:100.21464\n",
      "[60]\tvalidation_0-rmse:100.71124\n",
      "[61]\tvalidation_0-rmse:101.34091\n",
      "[62]\tvalidation_0-rmse:102.06951\n",
      "[63]\tvalidation_0-rmse:104.74359\n",
      "[64]\tvalidation_0-rmse:104.68056\n",
      "[65]\tvalidation_0-rmse:104.75848\n",
      "[66]\tvalidation_0-rmse:105.13770\n",
      "[67]\tvalidation_0-rmse:105.00198\n",
      "[68]\tvalidation_0-rmse:104.60687\n",
      "[69]\tvalidation_0-rmse:104.00185\n",
      "[70]\tvalidation_0-rmse:103.98247\n",
      "[71]\tvalidation_0-rmse:103.80533\n",
      "[72]\tvalidation_0-rmse:103.85370\n",
      "[73]\tvalidation_0-rmse:103.89475\n",
      "[74]\tvalidation_0-rmse:103.78204\n",
      "[75]\tvalidation_0-rmse:104.53307\n",
      "[76]\tvalidation_0-rmse:104.29366\n",
      "[77]\tvalidation_0-rmse:104.19616\n",
      "[78]\tvalidation_0-rmse:104.12449\n",
      "[79]\tvalidation_0-rmse:104.32928\n",
      "[80]\tvalidation_0-rmse:104.58815\n",
      "[81]\tvalidation_0-rmse:104.57429\n",
      "[82]\tvalidation_0-rmse:104.18788\n",
      "[83]\tvalidation_0-rmse:104.27676\n",
      "[84]\tvalidation_0-rmse:104.14155\n",
      "[85]\tvalidation_0-rmse:103.73478\n",
      "[86]\tvalidation_0-rmse:103.61959\n",
      "[87]\tvalidation_0-rmse:103.18742\n",
      "[88]\tvalidation_0-rmse:104.39492\n",
      "[89]\tvalidation_0-rmse:104.50237\n",
      "[90]\tvalidation_0-rmse:104.36062\n",
      "[91]\tvalidation_0-rmse:104.19743\n",
      "[92]\tvalidation_0-rmse:105.51869\n",
      "[93]\tvalidation_0-rmse:105.55531\n",
      "[94]\tvalidation_0-rmse:105.80637\n",
      "[95]\tvalidation_0-rmse:105.94889\n",
      "[96]\tvalidation_0-rmse:105.62084\n",
      "[97]\tvalidation_0-rmse:105.09312\n",
      "[98]\tvalidation_0-rmse:105.10075\n",
      "[99]\tvalidation_0-rmse:105.10441\n",
      "XGBoost Metrics: RMSLE=0.6075, RMSE=105.1043, MAE=20.8328, MAPE=4492193280.0000\n"
     ]
    }
   ],
   "source": [
    "print(\"Training XGBoost...\")\n",
    "X_train = train_set[feature_cols]\n",
    "y_train = train_set['sales']\n",
    "X_val = val_set[feature_cols]\n",
    "y_val = val_set['sales']\n",
    "\n",
    "model = xgb.XGBRegressor(\n",
    "    objective='reg:squarederror',\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=6,\n",
    "    random_state=42,\n",
    "    eval_metric='rmse'\n",
    ")\n",
    "\n",
    "early_stop = xgb.callback.EarlyStopping(\n",
    "    rounds=10,\n",
    "    metric_name='rmse',\n",
    "    data_name='validation_0',\n",
    "    save_best=True\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "y_pred_val = model.predict(X_val)\n",
    "\n",
    "actual = np.clip(y_val, 0, None)\n",
    "predicted = np.clip(y_pred_val, 0, None)\n",
    "rmsle = np.sqrt(mean_squared_error(np.log1p(actual), np.log1p(predicted)))\n",
    "rmse = np.sqrt(mean_squared_error(actual, predicted))\n",
    "mae = mean_absolute_error(actual, predicted)\n",
    "mape = mean_absolute_percentage_error(actual + 1e-10, predicted + 1e-10)\n",
    "print(f\"XGBoost Metrics: RMSLE={rmsle:.4f}, RMSE={rmse:.4f}, MAE={mae:.4f}, MAPE={mape:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6ed7d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost submission created.\n"
     ]
    }
   ],
   "source": [
    "xgb_test_preds = model.predict(test[feature_cols])\n",
    "test['sales'] = xgb_test_preds\n",
    "test_copy = test.reset_index()\n",
    "submission = test_copy[['id', 'sales']].merge(sub[['id']], on='id', how='right').fillna({'sales': 0}).clip(lower=0)\n",
    "submission.to_csv(r\"D:\\DEPI\\CIS project\\submission_xgboost.csv\", index=False)\n",
    "print(\"XGBoost submission created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4020354b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, df in [('train', train_set), ('val', val_set), ('test', test)]:\n",
    "    df.select_dtypes(include=[np.number]).to_parquet(f\"D:\\DEPI\\CIS project\\{name}_processed.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "921bc9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_groups = train_set.groupby(['store_nbr', 'family'])\n",
    "val_groups = val_set.groupby(['store_nbr', 'family'])\n",
    "val_dates = pd.date_range('2017-07-16', '2017-08-15')\n",
    "test_dates = pd.date_range('2017-08-16', '2017-08-31')\n",
    "val_steps = len(val_dates)\n",
    "test_steps = len(test_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7189ea5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ARIMA...\n"
     ]
    }
   ],
   "source": [
    "print(\"Training ARIMA...\")\n",
    "arima_models = {}\n",
    "for key, group in train_groups:\n",
    "    try:\n",
    "        model = auto_arima(group['sales'], seasonal=False, max_p=5, max_q=5, trace=False, error_action='ignore')\n",
    "        arima_models[key] = model\n",
    "    except:\n",
    "        arima_models[key] = None\n",
    "arima_val_preds = {}\n",
    "for k, m in arima_models.items():\n",
    "    if m:\n",
    "        arima_val_preds[k] = m.predict(val_steps)\n",
    "    else:\n",
    "        arima_val_preds[k] = np.zeros(val_steps)\n",
    "arima_test_preds = {}\n",
    "for k, m in arima_models.items():\n",
    "    if m:\n",
    "        arima_test_preds[k] = m.predict(test_steps)\n",
    "    else:\n",
    "        arima_test_preds[k] = np.zeros(test_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a23c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "actuals = []\n",
    "preds = []\n",
    "for (s, f), g in val_groups:\n",
    "    actuals.extend(g['sales'].values)\n",
    "    preds.extend(arima_val_preds[(s, f)])\n",
    "actual = np.clip(actuals, 0, None)\n",
    "predicted = np.clip(preds, 0, None)\n",
    "rmsle = np.sqrt(mean_squared_error(np.log1p(actual), np.log1p(predicted)))\n",
    "rmse = np.sqrt(mean_squared_error(actual, predicted))\n",
    "mae = mean_absolute_error(actual, predicted)\n",
    "mape = mean_absolute_percentage_error(actual + 1e-10, predicted + 1e-10)\n",
    "print(f\"ARIMA Metrics: RMSLE={rmsle:.4f}, RMSE={rmse:.4f}, MAE={mae:.4f}, MAPE={mape:.4f}\")\n",
    "\n",
    "test_copy = test.reset_index()\n",
    "for (store, family), pred_values in arima_test_preds.items():\n",
    "    mask = (test_copy['store_nbr'] == store) & (test_copy['family'] == family)\n",
    "    test_copy.loc[mask, 'sales'] = pred_values[:mask.sum()]\n",
    "submission = test_copy[['id', 'sales']].merge(sub[['id']], on='id', how='right').fillna({'sales': 0}).clip(lower=0)\n",
    "submission.to_csv(r\"D:\\DEPI\\CIS project\\submission_arima.csv\", index=False)\n",
    "print(\"ARIMA submission created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4de04ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training SARIMA...\")\n",
    "sarima_models = {}\n",
    "for key, group in train_groups:\n",
    "    try:\n",
    "        model = auto_arima(group['sales'], seasonal=True, m=7, max_p=5, max_q=5, trace=False, error_action='ignore')\n",
    "        sarima_models[key] = model\n",
    "    except:\n",
    "        sarima_models[key] = None\n",
    "sarima_val_preds = {}\n",
    "for k, m in sarima_models.items():\n",
    "    if m:\n",
    "        sarima_val_preds[k] = m.predict(val_steps)\n",
    "    else:\n",
    "        sarima_val_preds[k] = np.zeros(val_steps)\n",
    "sarima_test_preds = {}\n",
    "for k, m in sarima_models.items():\n",
    "    if m:\n",
    "        sarima_test_preds[k] = m.predict(test_steps)\n",
    "    else:\n",
    "        sarima_test_preds[k] = np.zeros(test_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5913ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "actuals = []\n",
    "preds = []\n",
    "for (s, f), g in val_groups:\n",
    "    actuals.extend(g['sales'].values)\n",
    "    preds.extend(sarima_val_preds[(s, f)])\n",
    "actual = np.clip(actuals, 0, None)\n",
    "predicted = np.clip(preds, 0, None)\n",
    "rmsle = np.sqrt(mean_squared_error(np.log1p(actual), np.log1p(predicted)))\n",
    "rmse = np.sqrt(mean_squared_error(actual, predicted))\n",
    "mae = mean_absolute_error(actual, predicted)\n",
    "mape = mean_absolute_percentage_error(actual + 1e-10, predicted + 1e-10)\n",
    "print(f\"SARIMA Metrics: RMSLE={rmsle:.4f}, RMSE={rmse:.4f}, MAE={mae:.4f}, MAPE={mape:.4f}\")\n",
    "\n",
    "test_copy = test.reset_index()\n",
    "for (store, family), pred_values in sarima_test_preds.items():\n",
    "    mask = (test_copy['store_nbr'] == store) & (test_copy['family'] == family)\n",
    "    test_copy.loc[mask, 'sales'] = pred_values[:mask.sum()]\n",
    "submission = test_copy[['id', 'sales']].merge(sub[['id']], on='id', how='right').fillna({'sales': 0}).clip(lower=0)\n",
    "submission.to_csv(r\"D:\\DEPI\\CIS project\\submission_sarima.csv\", index=False)\n",
    "print(\"SARIMA submission created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09aa71e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Prophet...\")\n",
    "prophet_models = {}\n",
    "for key, group in train_groups:\n",
    "    df = group.reset_index()[['date', 'sales']].rename(columns={'date': 'ds', 'sales': 'y'})\n",
    "    m = Prophet(yearly_seasonality=True, weekly_seasonality=True, daily_seasonality=True)\n",
    "    m.fit(df)\n",
    "    prophet_models[key] = m\n",
    "prophet_val_preds = {}\n",
    "for k, m in prophet_models.items():\n",
    "    prophet_val_preds[k] = m.predict(pd.DataFrame({'ds': val_dates}))['yhat'].values\n",
    "prophet_test_preds = {}\n",
    "for k, m in prophet_models.items():\n",
    "    prophet_test_preds[k] = m.predict(pd.DataFrame({'ds': test_dates}))['yhat'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144fae4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "actuals = []\n",
    "preds = []\n",
    "for (s, f), g in val_groups:\n",
    "    actuals.extend(g['sales'].values)\n",
    "    preds.extend(prophet_val_preds[(s, f)])\n",
    "actual = np.clip(actuals, 0, None)\n",
    "predicted = np.clip(preds, 0, None)\n",
    "rmsle = np.sqrt(mean_squared_error(np.log1p(actual), np.log1p(predicted)))\n",
    "rmse = np.sqrt(mean_squared_error(actual, predicted))\n",
    "mae = mean_absolute_error(actual, predicted)\n",
    "mape = mean_absolute_percentage_error(actual + 1e-10, predicted + 1e-10)\n",
    "print(f\"Prophet Metrics: RMSLE={rmsle:.4f}, RMSE={rmse:.4f}, MAE={mae:.4f}, MAPE={mape:.4f}\")\n",
    "\n",
    "test_copy = test.reset_index()\n",
    "for (store, family), pred_values in prophet_test_preds.items():\n",
    "    mask = (test_copy['store_nbr'] == store) & (test_copy['family'] == family)\n",
    "    test_copy.loc[mask, 'sales'] = pred_values[:mask.sum()]\n",
    "submission = test_copy[['id', 'sales']].merge(sub[['id']], on='id', how='right').fillna({'sales': 0}).clip(lower=0)\n",
    "submission.to_csv(r\"D:\\DEPI\\CIS project\\submission_prophet.csv\", index=False)\n",
    "print(\"Prophet submission created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90641d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training LSTM...\")\n",
    "seq_length = 7\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "for _, g in train_set.groupby(['store_nbr', 'family']):\n",
    "    g = g.sort_index()\n",
    "    for i in range(len(g) - seq_length):\n",
    "        X_train.append(g.iloc[i:i+seq_length][feature_cols].values)\n",
    "        y_train.append(g.iloc[i+seq_length]['sales'])\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "X_val = []\n",
    "y_val = []\n",
    "for _, g in val_set.groupby(['store_nbr', 'family']):\n",
    "    g = g.sort_index()\n",
    "    for i in range(len(g) - seq_length):\n",
    "        X_val.append(g.iloc[i:i+seq_length][feature_cols].values)\n",
    "        y_val.append(g.iloc[i+seq_length]['sales'])\n",
    "X_val = np.array(X_val)\n",
    "y_val = np.array(y_val)\n",
    "\n",
    "model = Sequential([LSTM(50, activation='relu', input_shape=(seq_length, len(feature_cols))), Dense(1)])\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=32, validation_data=(X_val, y_val), verbose=1)\n",
    "\n",
    "lstm_val_preds = model.predict(X_val).flatten()\n",
    "\n",
    "X_test = []\n",
    "for key in test.groupby(['store_nbr', 'family']).groups.keys():\n",
    "    last_seq = train_set.xs(key, level=['store_nbr', 'family']).tail(seq_length)[feature_cols].values\n",
    "    X_test.append(last_seq)\n",
    "X_test = np.array(X_test)\n",
    "\n",
    "test_preds_raw = model.predict(X_test).flatten()\n",
    "lstm_test_preds = {}\n",
    "for i, key in enumerate(test.groupby(['store_nbr', 'family']).groups.keys()):\n",
    "    group_size = len(test.xs(key[0], level='store_nbr').xs(key[1], level='family'))\n",
    "    lstm_test_preds[key] = np.full(group_size, test_preds_raw[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffce298",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual = np.clip(y_val, 0, None)\n",
    "predicted = np.clip(lstm_val_preds, 0, None)\n",
    "rmsle = np.sqrt(mean_squared_error(np.log1p(actual), np.log1p(predicted)))\n",
    "rmse = np.sqrt(mean_squared_error(actual, predicted))\n",
    "mae = mean_absolute_error(actual, predicted)\n",
    "mape = mean_absolute_percentage_error(actual + 1e-10, predicted + 1e-10)\n",
    "print(f\"LSTM Metrics: RMSLE={rmsle:.4f}, RMSE={rmse:.4f}, MAE={mae:.4f}, MAPE={mape:.4f}\")\n",
    "\n",
    "test_copy = test.reset_index()\n",
    "for (store, family), pred_values in lstm_test_preds.items():\n",
    "    mask = (test_copy['store_nbr'] == store) & (test_copy['family'] == family)\n",
    "    test_copy.loc[mask, 'sales'] = pred_values[:mask.sum()]\n",
    "submission = test_copy[['id', 'sales']].merge(sub[['id']], on='id', how='right').fillna({'sales': 0}).clip(lower=0)\n",
    "submission.to_csv(r\"D:\\DEPI\\CIS project\\submission_lstm.csv\", index=False)\n",
    "print(\"LSTM submission created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2250d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "store = 1\n",
    "family = 'AUTOMOTIVE'\n",
    "ts = train_set.loc[(store, family), 'sales'].sort_index()\n",
    "print(f\"Analyzing Store {store}, Family {family}...\")\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(ts, label='Original')\n",
    "plt.plot(ts.rolling(30).mean(), label='Rolling Mean', color='red')\n",
    "plt.plot(ts.rolling(30).std(), label='Rolling Std', color='black')\n",
    "plt.title(f\"Stationarity: Store {store}, Family {family}\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "result = adfuller(ts.dropna())\n",
    "print(f\"ADF Statistic: {result[0]:.4f}, p-value: {result[1]:.4f}\")\n",
    "\n",
    "ts_diff = ts.diff().dropna()\n",
    "plt.figure(figsize=(12, 6))\n",
    "plot_acf(ts_diff, lags=20)\n",
    "plt.title(f\"ACF of Differenced Series\")\n",
    "plt.show()\n",
    "\n",
    "lagged = pd.DataFrame({'sales': ts})\n",
    "for lag in [1, 7, 14, 21, 28]:\n",
    "    lagged[f'lag_{lag}'] = ts.shift(lag)\n",
    "print(\"Lagged Correlations:\\n\", lagged.corr()['sales'].drop('sales'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c039e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Execution completed.\")\n",
    "mem = psutil.Process(os.getpid()).memory_info().rss / 1024**2\n",
    "print(f\"Memory usage: {mem:.2f} MB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fair-chem",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
